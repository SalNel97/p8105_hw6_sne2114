---
title: "Homework 6"
author: "Salah El-Sadek"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(p8105.datasets)
library(viridis)
library(mgcv)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))

knitr::opts_chunk$set(comment = NA, message = FALSE, warning = FALSE, echo = TRUE)
```


## Problem 1


Read in and tidy the data.

```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```

Fitting Logistic Regression for one city, trying to predict resolution of case using victim age, race, and sex.

```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```

Running this same logistic model across cities.

```{r}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

Plotting ORs with 95 CI for each of the cities.

```{r}
models_results_df %>% 
  filter(term == "victim_raceWhite") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR, color = city_state)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none") +
  labs(
    title = "Odds Ratio with 95% CI by City",
    x = "City",
    y = "Odds Ratio") +
  scale_y_continuous(breaks = seq(0, 28, by = 1))
```

It seems like all the ORs are above 1 for every city, meaning the odds of solving a homicide for a white victim are better than the odds of solving a homicide for a black victim. Oakland, Omaha, Boston, and Pittsburgh are the cities with the highest ORs.


## Problem 2


Read in and tidy data set as well as change variables to factors as appropriate. We also checked that all 4342 entries for every variable returns false when checking for missing values.

```{r}
baby_df =
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
 
baby_df %>% is.na() %>% summary()

baby_df
```

I have hypothesised a model based on apriori hypotheses which focus on physical and growth attributes for the baby (body length, head circumference, gestational age) and weight attributes of the mother (pre-pregnancy weight, weight at delivery, weight gained during pregnancy) to predict birthweight for the baby. I also thought to include a delwt*wtgain interaction term since weight gained during pregnancy is related to mother's weight at delivery (buit not pre-preganncy weight).

```{r}
my_model = lm(bwt ~ blength + bhead + gaweeks + delwt + ppwt + wtgain + delwt*wtgain, data = baby_df)
summary(my_model)

baby_df %>% 
  add_residuals(my_model) %>% 
  add_predictions(my_model) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .4) +
  labs(
    title = "Apriori Model",
    x = "Predictions",
    y = "Residuals")
```

Prediction values seem to be concentrated between 2500 and 4000 while the residual values seem to aggregate between -600 and 1000.

We will compared our model to two other hypothesized models: one predicting birth weight using baby body length and gestational age main effects only, the other model using baby body length, baby head circumference, baby sex, and all possible interaction terms to predict birth weight.

```{r}
model_1 = lm(bwt ~ blength + gaweeks, data = baby_df)
summary(model_1)

baby_df %>% 
  add_residuals(model_1) %>% 
  add_predictions(model_1) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .4) +
  labs(
    title = "Model 1: Body Length and Gestational Age Main Effects Only",
    x = "Predictions",
    y = "Residuals")


model_2 = lm(bwt ~ (blength + bhead + babysex)^2 + blength*bhead*babysex, data = baby_df)
summary(model_2)

baby_df %>% 
  add_residuals(model_2) %>% 
  add_predictions(model_2) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .4) +
  labs(
    title = "Model 2: Body Length, Head Circumference, Sex, and All Possible Interactions ",
    x = "Predictions",
    y = "Residuals")
```

We can already visually see that the scatter plots for model 1 and model 2 are more aggregated and compact that the plot for my model (the majority of residuals and prediction values seem to be condensed within a smaller range of values than in my model).

We will compare all 3 models with cross validation of average RMSEs.

```{r}
cv_df =
  crossv_mc(baby_df, 100)

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble

cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

cv_df =
  cv_df %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
cv_df =
  cv_df %>%
  mutate(my_model = map(train, ~lm(bwt ~ blength + bhead + gaweeks + delwt + ppwt + wtgain + delwt*wtgain, data = .x)),
         model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model_2 = map(train, ~lm(bwt ~ (blength + bhead + babysex)^2 + blength*bhead*babysex, data = .x))
         ) %>%
  mutate(
    rmse_1 = map2_dbl(.x = my_model, .y = test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(.x = model_1, .y = test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(.x = model_2, .y = test, ~rmse(model = .x, data = .y))
  )

cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = recode(model, 
                        "1" = "My Model", 
                        "2" = "Main Effects Model",
                        "3" = "All Interactions Model")) %>% 
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin() +
  labs(
    title = "Cross Validation of Average RMSE for Each Model",
    x = "Model",
    y = "RMSE") +
   scale_y_continuous(breaks = seq(200, 400, by = 25))
```

We can see that my model has the lowest RMSE on average making it the best predictor model out of the 3 discussed.


## Problem 3


Read in and tidy the data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

weather_df
```



```{r}
bootstrap_df = 
  weather_df %>%
  bootstrap(5000, id = "strap_id") %>%
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    results_1 = map(models, broom::glance),
    results_2 = map(models, broom::tidy)
    ) %>%
  select(strap_id, results_1, results_2) %>%
  unnest(results_1, results_2) %>%
  pivot_wider(
    names_from = term, 
    values_from = estimate
    ) %>%
  group_by(strap_id) %>%
  summarise_each(funs(first(.[!is.na(.)]))) %>%
  rename(intercept = '(Intercept)') %>%
  mutate(
    LOG = log(intercept * tmin)
    ) %>%
  select(strap_id, r.squared, intercept, tmin, LOG)

bootstrap_df
```



```{r}
bootstrap_df %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
   title = "Distribution of r-squared",
   x = "r-squared",
   y = "Density")


bootstrap_df %>% 
  ggplot(aes(x = LOG)) +
  geom_density() +
  labs(
   title = "Distribution of Log (B1*B2)",
   x = "Log (B1*B2)",
   y = "Density")
```



```{r}
bootstrap_df %>% 
  pull(r.squared) %>% 
  quantile(c(0.025, 0.975))
```



```{r}
bootstrap_df %>% 
  pull(LOG) %>% 
  quantile(c(0.025, 0.975))
```

